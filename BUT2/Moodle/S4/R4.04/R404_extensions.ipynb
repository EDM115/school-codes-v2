{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> R1.04 Méthodes d'optimisation <br> Extensions possibles </center>\n",
    "<center> 2023/2024 - Tom Ferraut, Thibault Godin & Lucie Naert </center>\n",
    "<center> IUT de Vannes, BUT Informatique </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions suggérées :\n",
    "\n",
    "\n",
    "#### Adapatative learning rate :\n",
    "Dans tous nos exemples, le pas $d$ (appelé taux d'apprentissage en machine learning) reste constant.\n",
    "Il peut-être intéressant d'adapter le pas pour partir avec des grandes valeurs que l'on dimiunue au fur et à mesure.\n",
    "\n",
    "Par exemple on peut choisir à l'étape $i$ le pas $d[i] = d[0]/(1+i)$\n",
    "\n",
    "Tester différentes stratégies d'adaptation du pas et les illustrer par des exemples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Gradient avec inertie (momentum)\n",
    "\n",
    "\n",
    "L'idée de cette variante est de garder ne mémoire l'étape précédente.\n",
    "\n",
    "\n",
    "<u>_Methode du gradient avec inertie_</u>\n",
    "\n",
    "\n",
    "_Inputs_   $f, f', w_0 \\in R^d, x_{-1} = x_0$\n",
    "\n",
    " \n",
    "Pour $k = 0, 1, ...$\n",
    "\n",
    "1. Calculer une taille de pas $\\alpha_k > 0$ et un paramètre $\\beta_k > 0$\n",
    "2. Definir le nouveau point comme : $x_{k+1} = x_k − \\alpha_k f'(x_k + \\beta_k(x_k - x_{k-1})) + \\beta_k(x_k - x_{k−1})$ \n",
    "\n",
    "Fin\n",
    "\n",
    "Implémenter cette méthode et la comparer avec la méthode standard dans des exemples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Extensions suggérées :\n",
    "\n",
    "\n",
    "#### Choix du pas  adaptatif via une recherche linéaire \n",
    "\n",
    "Une méthode possible pour choisir le pas est la méthode du _backtracking_:\n",
    "\n",
    "supposons que l'on souahaite optimiser une fonction $f$.\n",
    "\n",
    "À l'étape $k$ le point courant est $a_k$, et la direction donnée par le gradient est $\\nabla f(a_k)$\n",
    "\n",
    "On choisit alors le pas $\\delta_{k+1}$ en utilisant l'algorithme suivant :\n",
    "\n",
    "\n",
    "$\\delta_{k+1} = \\delta_0$\n",
    "\n",
    "\n",
    "_Tant que_ $f(a_k - \\delta_{k+1} \\nabla f(a_k)) >  f(a_k)$\n",
    "\n",
    "\n",
    "\n",
    "$\\delta_{k+1} \\leftarrow \\frac{\\delta_{k+1} }{2}$\n",
    "\n",
    "\n",
    "\n",
    "On peut rafiner cette approche avec par exemple la condition dite d’Armijo\n",
    "\n",
    "#### Gradient coordonné\n",
    "L'idée est de ne pas calculer à chaque étape le gradient complet, mais seulement pour une coordonnée (choisie au hasard, ou selon un ordre prédéterminé)\n",
    "\n",
    "\n",
    "<u>_Methode du gradient coordonné_</u>\n",
    "\n",
    "\n",
    "_Inputs_   $f=(f_1,...,f_n), gradf=(gradf1,...,gradfn), v_0 \\in R^d$\n",
    "\n",
    " \n",
    "Pour $k = 0, 1, ...$\n",
    "\n",
    "1. Choisir une coordonnée $x_i$\n",
    "2. Definir le nouveau point comme : $v_{k+1} = v_k − \\alpha_k \\frac{\\partial f(v_k)}{\\partial x_i} $\n",
    "\n",
    "Fin\n",
    "\n",
    "Implémenter cette méthode et la comparer avec la méthode standard avec des exemples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions suggérées :\n",
    "\n",
    "\n",
    "#### Améliorer la visualisation de l'évolution de la droite de regression au cours de l'algorithme  :\n",
    "\n",
    "Vous pouvez tracer l'erreur ou utiliser un widget du même type que dans le TP1a :\n",
    "from ipywidgets import interact, fixed\n",
    "interact(...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Normalisation\n",
    "\n",
    "\n",
    "sklearn propose une option <tt> StandardScaler()</tt>, utilisée dans l'imense majorité des exemples disponible sur le net. Expliquer le fonctionnement et le but de cette option, et illustrer ses effets à l'aide d'exemples (on pourra utiliser des outils venant du cours de statisque R2.09 et du cours de probabilité R3.08)\n",
    "\n",
    "#### Augmentation des dimensions\n",
    "Utiliser la méthode du gradient pour étudier l'influence du nombre d'heures de travail et du nombre de machine par heure sur la production d'une entreprise, à partir des données suivantes :\n",
    "\n",
    "| Objs        | Work (hours)           | Machine/hour  |   Production ( 100 tons) |\n",
    "| ------------- |:-------------:| -----:|  -----:|\n",
    "| 1     | 1100 |  300 | 60  |\n",
    "| 2     | 1200 |  400 | 120  |\n",
    "| 3     | 1430 |  420 | 190  |\n",
    "| 4     | 1500 |  400 | 250  |\n",
    "| 5     | 1520 |  510 | 300  |\n",
    "| 6     | 1620 |  590 | 360  |\n",
    "| 7     | 1800 |  600 | 380  |\n",
    "| 8     | 1820 |  630 | 430  |\n",
    "| 9     | 1800 |  610 | 440  |\n",
    "\n",
    "\n",
    "\n",
    "On cherche donc a obtenir une formule du type :\n",
    "\n",
    "$$P= \\alpha_1 W + \\alpha_2 Mh + \\beta $$\n",
    "\n",
    "Dont l'erreur est donnée par \n",
    "\n",
    "$$E(\\alpha_1,\\alpha_2,\\beta) =  \\sum_i (P_i - (\\alpha_1W_i + \\alpha_2Mh_i + \\beta)$$\n",
    "\n",
    "\n",
    "#### Gradient stochatisque (version regression linéaire)\n",
    "L'idée est de ne pas calculer à chaque étape le gradient complet, mais seulement pour un exemple $(X_i,y_i) (choisie au hasard, ou selon un ordre prédéterminé)\n",
    "\n",
    "\n",
    "<u>_Methode du gradient stochastique_</u>\n",
    "\n",
    "\n",
    "_Inputs_   $X=(X_1,...,X_n), y=(y_1,...,y_n), [a_0,b_0] \\in R^2$\n",
    "\n",
    " \n",
    "Pour $k = 0, 1, ...$\n",
    "\n",
    "1. Choisir un couple $X_i,y_i)$\n",
    "2. Definir le nouveau point comme : $[a_{n+1},b_{n+1}] =  [a_{n+1},b_{n+1}]  - d [2a_{n+1}(a_nX_i + b_n -y_i),2(a_nX_i + b_n -y_i)]$ \n",
    "\n",
    "\n",
    "Fin\n",
    "\n",
    "Implémenter cette méthode et la comparer avec la méthode standard avec des exemples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\mathbf{SAT}$-Solver\n",
    "\n",
    "### Extensions suggérées :\n",
    "\n",
    "\n",
    "#### Génération aléatoire de grilles (résolubles)\n",
    "\n",
    "à l'aide des algorithmes précédents, écrire une fonction engendrant des grilles partielles de sudoku (résolubles)\n",
    "\n",
    "#### Étude statistique pour des sudoku $n \\times n$\n",
    "\n",
    "En partant de grilles aléatoires $n \\times n$, étudier le temps de résolution moyen (ou le nombre moyen de grilles résolubles, ou tout autre indicateur dont vous expliquerez la pertinence) \n",
    "\n",
    "#### 3-Coloriage\n",
    "\n",
    "De nombreux problèmes importants en informatique (les problèmes inclus dans $\\mathbf{NP}$ sont _réductibles_ à 3-SAT, c'est-à-dire qu'on peut les convertir en un problème 3-SAT et vis-et-versa (en temps polynomial, avec un nombre de d'inconnues polynomial). \n",
    "\n",
    "On peut donc (essayer de) résoudre de nombreux problème avec 3-SAT. Parmi eux la 3-coloration (étant donné un graphe $G$, admet-il une 3 coloration ?)\n",
    "\n",
    "Écrire un programme qui, étant donné un graphe $G$ transforme 3-col en 3-SAT, résout le prolème à l'aide d'un SAT-solver, puis donne la solution en terme de graphe).\n",
    "\n",
    "\n",
    "_note :_ Pour le sens inverse, vous pouvez vous référer à https://cgi.csc.liv.ac.uk/~igor/COMP309/3CP.pdf par exemple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation \n",
    "### Extensions suggérées :\n",
    "\n",
    "\n",
    "#### explication des données et overfitting\n",
    "\n",
    "Une manière d'utiliser l'interpolation comme un outil (naïf et imparfait) d'apprentissage d'IA est de partager les données entre données d'entrainement et données de validation (train et test). Si l'erreur $\\sum_test (f(x_i) -y_i)^2 $ est faible alors on a un bon modèle.\n",
    "\n",
    "Illustrer ce principe sur des exemples.\n",
    "\n",
    "_remarque :_ même si l'interpolation n'est pas à proprement parler une méthode de machine learning, cette méthode d'analyse qualitative de l'approximation est centrale en apprentissage. \n",
    "\n",
    "#### phénomène de Runge\n",
    "\n",
    "L'interpolation d'une fonction peut donner des résultats abhérents, ou toutefois déroutant.\n",
    "\n",
    "Un exemple est le [phénomène de Runge](https://fr.wikipedia.org/wiki/Ph%C3%A9nom%C3%A8ne_de_Runge)\n",
    "\n",
    "L'illustrer avec python, et des exemple ne provenant pas de wikipedia.\n",
    "\n",
    "#### interpolation de Hermite\n",
    "\n",
    "On peut essayer de construire une fonction qui ressemble à $f$ aux points $x_i$, mais aussi à $f'$. Cela conduit à [l'interpolation de Hermite](https://fr.wikipedia.org/wiki/Interpolation_d'Hermite)\n",
    "\n",
    "L'illustrer avec python, et des exemples ne provenant pas de wikipedia.\n",
    "\n",
    "##### Extensions suggérées :\n",
    "\n",
    "#### Corrections d'erreur dans le cas où on a $t$ erreurs exactement\n",
    "Faire de la correction d'erreur dans le cas où il y a autant d'erreurs que de lettres ajoutées en fin de message.\n",
    "\n",
    "\n",
    "#### corps finis\n",
    "\n",
    "Les fonctions présentées sont normalement implémentées sur des corps finis ($\\mathbb{Z} / \\mathbb{pZ}$, comme vu en crypto).\n",
    "\n",
    "Adapter vos fonctions et présenter des exemples. Justifier l'iteret de travailler sur ces corps finis\n",
    "\n",
    "#### codes de Reed-Solomon\n",
    "\n",
    "Comprendre et implémenter (une version éventuellement simplifiée) des codes de Reed-Solomon (une\n",
    "[bonne référence](http://www.fa17.eecs70.org/static/notes/n9.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
