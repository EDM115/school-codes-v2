{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> R1.04 Méthodes d'optimisation <br> TP1a - Descente de gradient 1D </center>\n",
    "<center> 2023/2024 - Tom Ferragut & Thibault Godin & Lucie Naert </center>\n",
    "<center> IUT de Vannes, BUT Informatique </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP est d'implémenter quelques méthodes numériques d'optimisation, basées sur la descente de gradient, dont le cœur est résumé par :\n",
    "\n",
    "$$a_{k+1} = a_k -  \\delta f'(a_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Affichage graphique\n",
    "\n",
    "%matplotlib notebook \n",
    "# enable interactivity of plots\n",
    "# attention a bien fermer les fenetres plt après chaque visualisalgèbreation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 : Optimisation de la fonction $f: x \\mapsto x^2 +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode (la plus basique) de descente de gradient, en 1D.\n",
    "\n",
    "\n",
    "On va considérer la fonction $f: x \\mapsto x^2 +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** \n",
    "\n",
    "Calculer $f'$ et dresser le tableau de variation de $f$.\n",
    "\n",
    "En déduire que $f$ admet un unique minimum sur $\\mathbb{R}$ et le donner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** \n",
    "\n",
    "Écrire une fonction `gradDesc_ex1(x0,k,d)` qui implémente la descente de gradient pour la fonction $f$ ci-dessus (on considère que la dérivée est connue). x0 étant l'abscisse de départ, k le nombre d'itérations et d représentant le pas $\\delta$.\n",
    "\n",
    "_Note : pour débugger, il peut être interessant d'afficher les résultats intermédiaires (ajout possible d'un paramètre booléen <tt>verbose</tt>)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc_ex1(x0,k,d):\n",
    "    return #TODO\n",
    "\n",
    "\n",
    "print(\"Minimum atteint en \", gradDesc_ex1(1,10,0.2)) #Est-ce cohérent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Ecrire une fonction `gradDescVect_ex1` dans laquelle vous modifierez la fonction précédente <tt>gradDesc_ex1</tt> afin de renvoyer deux vecteurs : le premier listant les points $x$ visités par l'algorithme, le deuxième les valeurs prises par le gradient en chaque point.\n",
    "\n",
    "La fonction `graphique_descente_ex1` utilise le résultat de <tt>gradDescVect_ex1</tt> pour donner une représentation graphique de la descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescVect_ex1(x0,k,d):\n",
    "    #TODO\n",
    "    return liste_X, liste_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphique_descente_ex1(x0,k,d):\n",
    "    ##inspiré de A. BODIN et F. RECHER    \n",
    "    xmin, xmax = -1.5*x0, 1.5*x0\n",
    "    plt.axis([xmin,xmax,-1,x0**2+2])\n",
    "     \n",
    "    num = 100\n",
    "    VX = np.linspace(xmin, xmax, num)\n",
    "    #1 Graphe de la fonctions\n",
    "    VY = VX**2+1\n",
    "    plt.plot(VX,VY,color='blue')\n",
    "\n",
    "    # 2. Points et gradients sur l'axe\n",
    "    liste_X, liste_grad = gradDescVect_ex1(x0,k,d)\n",
    "    for x in liste_X:    # points\n",
    "        plt.scatter(x, 0, color='red')\n",
    "    \n",
    "    for i in range(len(liste_X)-1):    # flèches\n",
    "        plt.arrow(liste_X[i],0, -d*liste_grad[i],0, linewidth=1, color='orange', length_includes_head=True, head_width=0.1, head_length=0.1)\n",
    "\n",
    "    # 3. Points et gradients sur le graphe\n",
    "    for x in liste_X:    # points\n",
    "        plt.scatter(x, x**2+1, color='red')\n",
    "    \n",
    "    for i in range(len(liste_X)-1):    # flèches\n",
    "        plt.arrow(liste_X[i],liste_X[i]**2+1, -d,-d*liste_grad[i], linewidth=1, color='orange', length_includes_head=True, head_width=0.05, head_length=0.1)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return -1\n",
    "\n",
    "\n",
    "graphique_descente_ex1(x0 = 1, k = 10, d = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Optimisation d'une fonction dérivable $f$ dont on connait la dérivée $f'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "1. Ecrire une fonction `gradDesc_ex2(f,fp,x0,k,d)` qui produit le même résultat que <tt>gradDesc_ex1</tt> mais pour une fonction quelconque <tt>f</tt> dont on connait la dérivée <tt>fp</tt>. \n",
    "\n",
    "2. Remarquer que les résultats obtenus sont bien les mêmes que pour l'exercice 1\n",
    "\n",
    "3. Ecrire une fonction `gradDescVect_ex2(f,fp,x0,k,d)` qui produit les mêmes résultats que <tt>gradDescVect_ex1</tt> mais pour une fonction quelconque <tt>f</tt> dont on connait la dérivée <tt>fp</tt>. \n",
    "\n",
    "La fonction `graphique_descente_ex2` utilise le résultat de <tt>gradDescVect_ex2</tt> pour donner une représentation graphique de la descente de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc_ex2(f,fp,x0,k,d):\n",
    "    #TODO\n",
    "    return #TODO\n",
    "\n",
    "def f(x):\n",
    "    return #TODO\n",
    "\n",
    "def fp(x):\n",
    "    return #TODO\n",
    "\n",
    "\n",
    "print(\"Obtient-on les mêmes résultats pour le cas x^2 +1 ?\", gradDesc_ex2(f,fp,1,10,0.2)==gradDesc_ex1(1,10,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescVect_ex2(f,fp,x0,k,d):\n",
    "    #TODO\n",
    "    return liste_X, liste_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphique_descente_ex2(f,fp,x0,k,d):\n",
    "    ##inspiré de A. BODIN et F. RECHER    \n",
    "    xmin, xmax = -3*x0, 3*x0\n",
    "    plt.axis([xmin,xmax,-2,2])\n",
    "    \n",
    "    xmin, xmax = -5*x0, 5*x0\n",
    "    plt.axis([xmin,xmax,-2*f(x0),2*f(x0)])\n",
    "     \n",
    "    num = 100\n",
    "    VX = np.linspace(xmin, xmax, num)\n",
    "    #1 Graphe de la fonctions\n",
    "    VY =f(VX)\n",
    "    plt.plot(VX,VY,color='blue')\n",
    "\n",
    "    # 2. Points et gradients sur l'axe\n",
    "    liste_X, liste_grad = gradDescVect_ex2(f,fp,x0,k,d)\n",
    "    for x in liste_X:    # points\n",
    "        plt.scatter(x, 0, color='red')\n",
    "    \n",
    "    for i in range(len(liste_X)-1):    # flèches\n",
    "        plt.arrow(liste_X[i],0, -d*liste_grad[i],0, linewidth=1, color='orange', length_includes_head=True, head_width=0.05, head_length=0.1)\n",
    "\n",
    "    # 3. Points et gradients sur le graphe\n",
    "    for x in liste_X:    # points\n",
    "        plt.scatter(x, f(x), color='red')\n",
    "    \n",
    "    for i in range(len(liste_X)-1):    # flèches\n",
    "        plt.arrow(liste_X[i],f(liste_X[i]), -d,-d*liste_grad[i], linewidth=1, color='orange', length_includes_head=True, head_width=0.05, head_length=0.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    return -1\n",
    "\n",
    "graphique_descente_ex2(f,fp,1,10,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2 : Exemple de problème d'optimisation__\n",
    "\n",
    "Un industriel cherche à optimiser la quantité de métal utilisée pour la fabrication d’une boite de conserve. Nous avons vu en début de cours que la surface $S$ de métal utilisée dépendait du rayon $r$ de la boite (nous fixons le volume à 425mL).\n",
    "\n",
    "$$S(r) = 2\\pi r^2 + \\frac{2V}{r}$$\n",
    "\n",
    "En utilisant la descente de gradient et les fonctions définies précédemment, donner une approximation du rayon permettant d'utliser le moins de métal possible pour produire la boite. A quelle surface ce rayon correspond-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return #TODO\n",
    "\n",
    "\n",
    "def fp(x):\n",
    "    return #TODO\n",
    "\n",
    "x0 = 1\n",
    "k = 20\n",
    "d = 0.02\n",
    "rMin = gradDesc_ex2(f, fp, x0, k, d)\n",
    "\n",
    "print(\"Le rayon minimal trouvé en\", k, \"pas est de\", rMin)\n",
    "print(\"La surface correspondante est égale à\", f(rMin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** \n",
    "\n",
    "À l'aide des widgets suivant, observer l'influence des paramètres $k$ et $d$. Commentez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def fp(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "interact(graphique_descente_ex2,f=fixed(f),fp=fixed(fp), x0=fixed(1), k=(0, 50, 1), d=(0., .5, 0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(graphique_descente_ex2,f=fixed(f),fp=fixed(fp), x0=fixed(1), k=(0, 50, 1), d=(0., 4, 0.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Influence de k :__\n",
    "TODO\n",
    "\n",
    "__Influence de d :__\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Cas d'une fonction dont on ne connait pas la dérivée\n",
    "\n",
    "Le but de cette dernière partie est de proposer un algorithme pour utiliser la méthode du gradient\n",
    "\n",
    "\n",
    "Pour cela, on utilise :\n",
    "\n",
    "$$f'(x) = \\lim_{t \\to 0} \\frac{f(x)-f(x+t)}{t} \\approx \\frac{f(x)-f(x+\\varepsilon)}{\\varepsilon}$$\n",
    "\n",
    "\n",
    "_Remarque stabilité numérique :  on prend généralement un petit epsilon que l'on fixe, par exemple $\\varepsilon = 10^{-5}$ et l'on symétrise, on choisira donc :_\n",
    "$$f'(x)  \\approx \\frac{f(x+\\varepsilon)-f(x-\\varepsilon)}{2\\varepsilon}$$\n",
    "\n",
    "\n",
    "\n",
    "_Remarque python_ : on pourra utiliser les lambda expressions pour recycler le code précédent https://www.w3schools.com/python/python_lambda.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ecrire une fonction `fp_approx` qui donne l'approximation de la dérivée au point x en utilisant la formule précédente. \n",
    "\n",
    "2. Pour la fonction donnée, écrire la fonction `fp` donnant sa dérivée exacte. Comparer les résultats en un point.\n",
    "\n",
    "3. Faire la descente de gradient en utilisant <tt>fp_approx</tt> pour trouver le minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "def fp_approx():\n",
    "    #TODO\n",
    "    return #TODO\n",
    "\n",
    "def f(x):\n",
    "    return (x**2 - 11)**2 + (x - 7)**2\n",
    "\n",
    "#dérivée -14 - 42*x + 4*x**3\n",
    "def fp(x):\n",
    "    return #TODO\n",
    "\n",
    "print(\"Difference entre la derivée exacte et l'approximation :\",fp(1)-fp_approx())\n",
    "\n",
    "xMin = gradDesc_ex2(f, lambda x : fp_approx(), 1, 20, 0.01)\n",
    "\n",
    "print(\"Minimum\",f(xMin), \"trouvé en\", xMin)\n",
    "# min{(x^2 - 11)^2 + (x - 7)^2}≈13.2727 at x≈3.3957\n",
    "\n",
    "graphique_descente_ex2(f,lambda x : fp_approx(),1,20,.01,False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
