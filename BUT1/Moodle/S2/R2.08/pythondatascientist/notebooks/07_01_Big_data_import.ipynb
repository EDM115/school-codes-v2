{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 7 - Passage au big data (1ère partie)\n",
    "\n",
    "**Dans le cadre de ce Notebook, nous allons parler de l'environnement Apache Hadoop. Ce notebook n'est donc pas applicable dans votre environnement \"classique\".**\n",
    "\n",
    "**Pour que le code foctionnne, il vous faut un environnement Hadoop accessible depuis votre machine.**\n",
    "\n",
    "**N'essayez pas de faire fonctionner les cellules si votre environnemnt n'est pas correctement paramétré.**\n",
    "\n",
    "## 7. 1 Est-ce qu’on change tout quand on parle de big data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python est souvent considéré comme le langage du big data. Ceci vient du fait\n",
    "que la notion de big data est souvent mal définie par beaucoup d’utilisateurs. \n",
    "\n",
    "On a pu voir que Python est un langage bien adapté au traitement de la donnée et au machine learning grâce à sa simplicité, ses principes de fonctionnement et toutes les API disponibles. \n",
    "\n",
    "Dans le cadre du big data, on est exactement dans le même cas de figure qu’en deep learning. Vous ne trouverez aucun environnement big data nativement développé en Python. Ils sont majoritairement développés en Java. Cependant,\n",
    "Python sera très souvent un langage pour lequel il existe des API assez poussées.\n",
    "\n",
    "Les principes du langage Python restent les mêmes mais les commandes, fonctions\n",
    "et actions dépendront de l’API utilisée et donc du package sollicité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilisera ici Python pour récupérer des données sur un cluster Hadoop. Nous parlerons de Apache Spark dans le prochain Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Récupérer des données avec Python\n",
    "### 7.3.1 Les approches classiques\n",
    "Apache Hadoop est donc un environnement big data écrit en Java donnant accès à\n",
    "un système de fichiers distribués appelé HDFS et à des outils permettant d’extraire des\n",
    "informations de ces données.\n",
    "\n",
    "Nous sommes donc dans un cas, où nos données sont stockées dans des formats\n",
    "dits NoSQL (Not Only SQL) et où vous désirez les récupérer pour vos analyses en\n",
    "Python. \n",
    "\n",
    "Si vous passez par Python, vous allez charger en mémoire les données que\n",
    "vous récupérez de votre infrastructure. Ainsi, si ces données sont massives, vous allez très rapidement vous trouver face à un problème de taille : la capacité de votre machine en termes de mémoire vive.\n",
    "\n",
    "Il faut donc faire en sorte de ne charger sur votre machine que les données utiles et, réfléchir à la mise en place d’infrastructures plus puissantes. Par exemple, un serveur JupyterHub extrêmement puissant qui pourra traiter des masses de données plus importantes que votre machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Se connecter aux fichiers HDFS en Python – Utilisation de PyArrow\n",
    "Le système de fichiers HDFS est accessible en utilisant la commande hdfs depuis\n",
    "le terminal. Néanmoins, cet outil est codé en Java et ne possède pas nativement\n",
    "d’API Python.\n",
    "\n",
    "La seule solution simple adaptée à Python 3 est liée au projet Apache Arrow. Il s’agit d’un environnement visant à unifier le traitement en mémoire de données colonnes.\n",
    "\n",
    "Pour utiliser Arrow, nous allons installer son API Python qui est dans le package PyArrow. \n",
    "\n",
    "Nous le récupérons grâce à Anaconda :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que vous l’avez installé dans votre environnement, vous pouvez vous\n",
    "connecter au système de fichiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import HdfsClient\n",
    "hdfs_alt = HdfsClient(host, port, username, driver='libhdfs3')\n",
    "with hdfs.open('/path/to/file') as f:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez utiliser les commandes qui se trouvent dans l’objet de la classe\n",
    "*HdfsClient* que vous avez créé pour faire des actions sur votre environnement\n",
    "Hadoop.\n",
    "\n",
    "Ceci vous permettra de gérer de nombreux types de fichiers, notamment les fichiers Apache Parquet de votre environnement big data.\n",
    "\n",
    "Le projet Apache Arrow est en pleine expansion et, selon les responsables de\n",
    "la fondation Apache, il devrait traiter près de 80 % des données big data dans les prochaines années."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 Faire du Hive avec Python – Utilisation de PyHive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hive est un autre projet de l’environnement big data qui est beaucoup plus ancien mais qui donne accès à une interface du type SQL pour requêter vos données stockées en environnement big data.\n",
    "\n",
    "Le package disponible pour faire des requêtes Hive en Python est pyHive. Vous\n",
    "pouvez l’installer dans votre environnement Anaconda avec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda pyhive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque vous avez installé PyHive, vous pouvez alors travailler directement sur vos tables Hive créées dans votre environnement Hadoop.\n",
    "\n",
    "On utilise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "cursor = hive.connect('localhost').cursor()\n",
    "cursor.execute('SELECT * FROM ma_base LIMIT 10')\n",
    "for row in cursor.fetchall():\n",
    "    print(row[0], row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l’appel de la méthode connect, on utilisera aussi les caractéristiques utilisateur de votre session Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code va vous permettre de charger des données directement sur votre machine\n",
    "et ainsi travailler sur ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les approches présentées jusqu’ici ne sont pas utilisées de manière généralisée.\n",
    "\n",
    "L’outil de big data employé par les data scientists, c’est Apache Spark. Il permet, entre autres, de charger des fichiers en Hive, mais surtout de faire du machine learning. Nous en parlons dans le prochain Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
