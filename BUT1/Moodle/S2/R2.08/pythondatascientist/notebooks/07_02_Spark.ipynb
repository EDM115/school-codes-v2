{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 7 - Passage au big data (2ème partie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dans le cadre de ce Notebook, nous allons parler de l'environnement Apache Spark. Ce notebook n'est donc pas applicable dans votre environnement \"classique\".**\n",
    "\n",
    "**Pour que le code foctionnne, il vous faut un environnement Spark correctement installé.**\n",
    "\n",
    "**N'essayez pas de faire fonctionner les cellules si votre environnemnt n'est pas correctement paramétré. Les cellules de code ont été passées au format RawNBConvert afin de ne pas rendre le Notebook inutilisable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark est un projet de la fondation Apache (actuellement dans sa version 3).\n",
    "\n",
    "Il a pour objectif de pallier les lacunes de Hadoop quant au traitement nécessitant de nombreux allers-retours.\n",
    "\n",
    "Si, malgré tous vos efforts, vous n’avez pas réussi à extraire des données de\n",
    "manière qu’elles tiennent dans votre mémoire RAM, le recours à une autre solution deviendra indispensable. Cette solution est Apache Spark.\n",
    "\n",
    "Cet environnement, développé à Berkeley, est un système de traitement distribué\n",
    "sur les noeuds d’une infrastructure big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez tester Spark, je vous conseille d'essayer la version gratuite de Databricks qui est simple d'accèes :\n",
    "\n",
    "https://databricks.com/signup#signup/community\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Le DataFrame de Spark SQL\n",
    "\n",
    "Nous allons nous concentrer sur Spark SQL. Ceci nous permettra d’introduire un objet : le DataFrame de Spark. Il s’agit d’un objet proche du RDD, mais qui permet de stocker de manière distribuée des données structurées, là où les RDD nous permettent de stocker des données non structurées.\n",
    "\n",
    "Il se rapproche très fortement du DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancer votre session Spark\n",
    "Commençons par lancer une session Spark en utilisant dans un premier temps le\n",
    "package findspark et la classe SparkSession de pyspark.sql :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe findspark\n",
    "import findspark\n",
    "# on initialise findspark pour identifier nos chemins Spark\n",
    "findspark.init()\n",
    "# on importe SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "# on crée une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "         .appName(\"Exemples avec Python et Spark SQL\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture des données (json, parquet, csv, hive)\n",
    "\n",
    "Spark vous permet de lire de nombreux types de données, que ce soit des données csv ou SQL classiques ou des données issues d’environnements big data. En voici quelques exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture d’un fichier json\n",
    "df = spark.read.json(\"data.json\")\n",
    "# lecture d’un fichier parquet\n",
    "df3 = spark.read.load(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permet aussi d’utiliser des données issues de fichiers csv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idf=spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "              .option(\"delimiter\",\";\")\\\n",
    "              .option(\"inferSchema\", \"true\")\n",
    "              .load(\"../Data/base-comparateur-de-territoires.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autre format important dans le cadre du big data est le format Hive. Pour se\n",
    "connecter à une base Hive et soumettre du code SQL, on utilisera :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# on crée une session avec accès à Hive\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# on peut afficher une base\n",
    "spark.sql('show databases').show()\n",
    "\n",
    "# on peut créer une base\n",
    "spark.sql('create database base1')\n",
    "\n",
    "# on peut faire des requêtes en SQL\n",
    "spark.sql(\"select * from table1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi transformer un DataFrame Pandas en DataFrame Spark en utilisant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipuler des DataFrames\n",
    "Il est très simple de manipuler des DataFrames de différentes manières. Spark part du principe que les calculs ne sont pas effectués chaque fois que vous soumettez du code. \n",
    "\n",
    "Ils le sont lorsque vous demandez explicitement à Spark de faire les calculs ou\n",
    "d’afficher les résultats. Ces opérations de calcul ou d’affichage sont appliquées avec les méthodes .collect() ou .show().\n",
    "\n",
    "Nous allons manipuler les données sur les communes d’Île-de-France. Nous\n",
    "voulons extraire des informations de ces données sur les communes de la région\n",
    "Île-de-France.\n",
    "\n",
    "Les codes ci-dessous nous\n",
    "permettent d’effectuer la plupart des manipulations dont nous avons besoin :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère les données d’Ile-de-France\n",
    "# on a des titres dans la première ligne\n",
    "# le séparateur est le;\n",
    "# on demande à Spark d’inférer les types\n",
    "data_idf = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "           .option(\"delimiter\",\";\")\\\n",
    "           .option(\"inferSchema\", \"true\")\\\n",
    "           .load(\"../data/base-comparateur-de-territoires.csv\")\n",
    "# on peut afficher les 8 premiers noms de colonnes :\n",
    "data_idf.columns[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sélectionne une colonne et on affiche le résultat\n",
    "data_idf.select(\"LIBGEO\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les opérations ci-dessus sont stockées en mémoire et ne renvoient rien. C’est\n",
    "uniquement lorsqu’on ajoute show() ou collect() que les opérations sont\n",
    "effectuées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut filtrer les observations\n",
    "data_reduced.filter(data_reduced['startswith(LIBGEO, Paris)'] == True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons sélectionné uniquement les observations commençant par « Paris »,\n",
    "on obtient donc les 20 arrondissements et leurs populations.\n",
    "\n",
    "On peut alors sauver ces données sous forme de fichiers parquet ou json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced.select(\"P14_POP\",\" LIBGEO\").write.save(\"resultat.parquet\")\n",
    "data_reduced.select(\"P14_POP\",\"LIBGEO\").write.save(\"resultat.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Afficher des statistiques descriptives\n",
    "Spark permet aussi de calculer des statistiques sur les données en utilisant, par\n",
    "exemple, une opération groupby :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on utilise un groupBy par département et\n",
    "# on affiche le salaire médian moyen\n",
    "salaire_med_moy = data_idf.groupBy(\"DEP\").agg({\"MED14\" :\"mean\"})\n",
    "salaire_med_moy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut transformer le résultat en format Pandas\n",
    "salaire_med_moy_pandas = salaire_med_moy.toPandas()\n",
    "# on aura les sorties de Pandas\n",
    "salaire_med_moy_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses opérations proches de celles de Pandas sont disponibles avec\n",
    "Spark. \n",
    "\n",
    "\n",
    "#### Terminer votre session Spark\n",
    "Une fois que vous avez terminé de travailler sur votre session Spark, vous pouvez la fermer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Le machine learning avec Spark\n",
    "\n",
    "#### Préparation des données\n",
    "\n",
    "Nous supposons que nous avons déjà créé notre session Spark. Nous devons\n",
    "maintenant récupérer nos données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère les données telecom\n",
    "churn=spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"../Data/telecom.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La phase de préparation qui suit est importante. Il s’agit de définir les variables explicatives (x) et la variable cible (y) tout en transformant les variables non adaptées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe une classe qui transforme les colonnes qualitatives en colonnes\n",
    "# sous forme d’entiers (équivalent de LabelEncoder de Scikit-Learn)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# on va transformer la colonne Churn? et on va la nommer Churn2\n",
    "indexer = StringIndexer(inputCol='Churn?', outputCol='Churn2').fit(churn)\n",
    "\n",
    "# on construit ensuite un vecteur rassemblant toutes les colonnes explicatives\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# on rassemble la liste des colonnes numériques que l’on va utiliser\n",
    "numericCols = ['Day Mins','Day Calls','Day Charge','Eve Mins',\n",
    "               'Eve Calls','Eve Charge','Night Mins','Night lClas',\n",
    "               'Night Charge','Intl Mins','Intl Calls']\n",
    "\n",
    "# on crée un objet qui rassemble toutes ces colonnes dans une colonne\n",
    "# nommée var_expl\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"var_expl\")\n",
    "\n",
    "# on divise le DataFrame initial (churn) en deux DataFrame représentant\n",
    "# respectivement 70% et 30% des données\n",
    "\n",
    "(trainingData, testData) = churn.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À la différence de Scikit-Learn, on va devoir nommer les groupes de variables en\n",
    "entrée et en sortie lors de la création de l’objet à partir de la classe du modèle. \n",
    "\n",
    "Les données doivent donc avoir le format spécifié dans l’objet. Par ailleurs, on utilise un format spécifique pour les variables explicatives qui sont toutes stockées dans une structure à l’intérieur du DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du modèle et du pipeline\n",
    "Nous pouvons créer notre modèle de forêt aléatoire ainsi que le pipeline associé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# on crée notre modèle\n",
    "model=RandomForestClassifier(labelCol=\"Churn2\", featuresCol=\"var_expl\",\n",
    "                             numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# on construit le pipeline qui est composé des 3 étapes dévelopées auparavant\n",
    "pipeline = Pipeline(stages=[indexer, assembler, model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustement et validation du modèle\n",
    "Nous faisons les calculs sur les données d’apprentissage et testons sur les\n",
    "données de validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustement du modèle\n",
    "model = pipeline.fit(trainingData)\n",
    "# prévision sur les données de validation\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, Spark va créer de nouvelles colonnes dans nos données avec les\n",
    "prédictions (colonne prediction) et les probabilités de prédiction (colonne\n",
    "rawPrediction).\n",
    "\n",
    "Nous pouvons calculer des métriques comme l’AUC ou le pourcentage de bien\n",
    "classés (accuracy) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# cette classe calcule l’AUC de notre modèle\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
    "                                          labelCol=\"Churn2\")\n",
    "\n",
    "# on applique les données prédites à notre objet d’évalaution\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "# L’AUC est affichée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on calcule l’accuracy manuellement\n",
    "accuracy = predictions.filter(predictions.Churn2==predictions.prediction)\\\n",
    "                        .count() / float(testData.count())\n",
    "accuracy\n",
    "# on obtient l’accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les métriques utilisées nous permettent de voir que notre modèle ressemble à\n",
    "celui de Scikit-Learn en termes de performance (il est moins bon car nous avons\n",
    "moins de variables explicatives).\n",
    "\n",
    "Nous avons effectué tous les calculs dans notre environnement big data. Le seul\n",
    "moment où les données sont revenues vers nous est situé à la fin, pour récupérer le\n",
    "résultat.\n",
    "\n",
    "Cet exemple illustre bien la simplicité de Spark. L’utilisation de Spark pour des\n",
    "tâches plus complexes demande plus de travail mais PySpark et les DataFrames\n",
    "rendent ce passage très aisé pour un data scientist à l’aise avec les outils de traitement\n",
    "de données de Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
